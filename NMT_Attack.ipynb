{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NMT-Attack.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6xp_6IUfOOF0"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#NMT Attack\n",
        "\n",
        "Welcome to the NMT-Attack Notebook. Here we have detailed the process of reproducing our results for our paper : Lost in Translation\n",
        "\n",
        "We have 3 sections to this notebook. \n",
        "\n",
        "The first involves installation of dependencies.\n",
        "\n",
        "The second involves running the attack.\n",
        "\n",
        "The third involves analysis of our results.\n",
        "\n",
        "**Please upload the TextAttack folder to the files section of the notebook, situated on the left of the code section**\n",
        "\n",
        "\n",
        "##We also provide the provision to reproduce all 3 attacks we have used. \n",
        "\n",
        "For TextFooler, PWWS and TextBugger, please edit the models and tokenisers (as detailed in the NMT Attack section) in the **greedy_word_swap_wir.py** search methods file. \n",
        "\n",
        "For example, to work with Rotten Tomatoes dataset, please update the model and tokeniser to \n",
        "\n",
        "**original_tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-rotten-tomatoes\")**\n",
        "\n",
        "**original_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-rotten-tomatoes\")**\n",
        "\n",
        "**model = HuggingFaceModelWrapper(original_model,original_tokenizer)**\n",
        "\n",
        "\n",
        "This must be updated in both the colab notebook as well as the greedy_word_swap_wir.py file\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zMBhWbxFXJTf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnMtvyMzzam8"
      },
      "source": [
        "%cd /path/to/TextAttack/ #Run this instruction in every iteration of this notebook."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xp_6IUfOOF0"
      },
      "source": [
        "##Pip Requirements(Only run initially)\n",
        "\n",
        "You will in most cases have to restart runtime after this. Once you restart runtime, you DO NOT have to run this section again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW0tfwNGzkpZ"
      },
      "source": [
        "!pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5zKbW4TzkzQ"
      },
      "source": [
        "!pip install tensorflow_datasets\n",
        "!pip3 install -U easynmt\n",
        "# !pip install tensorflow-gpu==2.1.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LgdYIF4OhcW"
      },
      "source": [
        "!pip install captum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXPaYU9jOrYz"
      },
      "source": [
        "##NMT-Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3DhmKc_OYZi"
      },
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import textattack\n",
        "from textattack.models.wrappers import ModelWrapper\n",
        "from textattack.datasets import HuggingFaceDataset\n",
        "from textattack.attack_recipes import TextFoolerJin2019\n",
        "from textattack.attack_recipes import BAEGarg2019\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdNNjhKIPP53"
      },
      "source": [
        "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients, LayerDeepLiftShap, InternalInfluence, LayerGradientXActivation\n",
        "from captum.attr import visualization as viz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kuzlu58pSk1v"
      },
      "source": [
        "from textattack.datasets import HuggingFaceDataset\n",
        "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
        "from textattack.models.wrappers import ModelWrapper\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPI7ERWfPP8e"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Important Note\n",
        "\n",
        "The following code block specifies the particular model you would like to use for the attack, as well as the dataset to be used for the attack. \n",
        "\n",
        "###Make sure you specify the exact same tokenizer and model in the specific search method used by the algorithm. For convenience, we have already added a model in the search method of the algorithm for your convenience."
      ],
      "metadata": {
        "id": "GrWSG74dYfUv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6akPhnPP-v"
      },
      "source": [
        "dataset = HuggingFaceDataset(\"rotten_tomatoes\", None, \"test\")\n",
        "original_tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-rotten-tomatoes\")\n",
        "original_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-rotten_tomatoes\")\n",
        "model = HuggingFaceModelWrapper(original_model,original_tokenizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, chose the attack you would look to use. The options are:\n",
        "\n",
        "\n",
        "1.   TextFoolerJin2019\n",
        "2.   TextBuggerLi2018\n",
        "\n",
        "1.   PWWSRen2019\n",
        "\n",
        "\n",
        "Attach this to the build function, and run the cell.\n",
        "\n"
      ],
      "metadata": {
        "id": "GfooWadq7BZf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMj2KCw4PQDl"
      },
      "source": [
        "from textattack import Attacker\n",
        "from textattack.attack_recipes import TextBuggerLi2018\n",
        "attack = TextFoolerJin2019.build(model)\n",
        "attack_args = textattack.AttackArgs(num_examples=1000)\n",
        "attacker = Attacker(attack, dataset,attack_args)\n",
        "x=attacker.attack_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please store the last robust example array in the output, containing the number of robust examples for each examples, in the array below"
      ],
      "metadata": {
        "id": "IHbc5y2EhifW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rV9YaZD34mC"
      },
      "source": [
        "robust_ex=[1, 19, 1, 2, 1, 0, 18, 23, 0, 8, 1, 5, 3, 16] #Size of this array= number of attacks successful."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yieX2zBlNJI"
      },
      "source": [
        "countt=0\n",
        "for i in range(len(textfooler_yelp)):\n",
        "  if textfooler_yelp[i]!=0:\n",
        "    # print(org_list[i])\n",
        "    # print(i)\n",
        "    countt+=1;\n",
        "print(countt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analysis section"
      ],
      "metadata": {
        "id": "H5SUXFOUiRSW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6dD4E16iHJP"
      },
      "source": [
        "list_german=[]\n",
        "list_french=[]\n",
        "list_spanish=[]\n",
        "org_list=[]\n",
        "unadv=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7hRtjWUiJ_i"
      },
      "source": [
        "for i in x:\n",
        "  if(type(i)==textattack.attack_results.skipped_attack_result.SkippedAttackResult):\n",
        "    continue;\n",
        "  if(type(i)==textattack.attack_results.failed_attack_result.FailedAttackResult):\n",
        "    continue;\n",
        "  org_list.append(i.perturbed_text())\n",
        "  unadv.append(i.original_text())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJvruWFDiOYE"
      },
      "source": [
        "df_d=pd.DataFrame({\"original\":unadv,\n",
        "                 \"adversarial\":org_list,\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQMUIyT2ibU7"
      },
      "source": [
        "df_d.to_csv('/content/drive/MyDrive/labels/temp_results.csv') #store temporarily since the colab notebook might disconnect depending on GPU allocation provided"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge-KMlnvim6o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbRs2AAOZacx"
      },
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/labels/temp_results.csv')\n",
        "org_list= df['adversarial']\n",
        "unadv= df['original']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njDpLX7Epyoa"
      },
      "source": [
        "c=[]\n",
        "from nltk.tokenize import word_tokenize #to match tokenisation done during inference\n",
        "for t in org_list:\n",
        "  tokens = word_tokenize(t)\n",
        "  # remove all tokens that are not alphabetic\n",
        "  words = [word for word in tokens if word.isalpha()]\n",
        "  z=' '.join(words)\n",
        "  c.append(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNGiR_-6qJMz"
      },
      "source": [
        "org_list=c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4iLyrsLm9tW"
      },
      "source": [
        "trans_german=[]\n",
        "trans_french=[]\n",
        "trans_spanish=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKWNwRn5cZ4l"
      },
      "source": [
        "from easynmt import EasyNMT\n",
        "model_lang = EasyNMT('opus-mt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAtVY-qudGLy"
      },
      "source": [
        "list_german=model_lang.translate(org_list,source_lang='en', target_lang='de')\n",
        "list_french=model_lang.translate(org_list,source_lang='en', target_lang='fr')\n",
        "list_spanish=model_lang.translate(org_list,source_lang='en', target_lang='es')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_kHEzySdHZD"
      },
      "source": [
        "trans_german=model_lang.translate(list_german,source_lang='de', target_lang='en')\n",
        "trans_french=model_lang.translate(list_french,source_lang='fr', target_lang='en')\n",
        "trans_spanish=model_lang.translate(list_spanish,source_lang='es',target_lang='en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CzsxDvjj2uX"
      },
      "source": [
        "span=[]\n",
        "for i in range(len(trans_spanish)):\n",
        "  span.append(model([trans_spanish[i]]))\n",
        "st=[]\n",
        "for i in range(len(trans_spanish)):\n",
        "  st.append(model([trans_german[i]]))\n",
        "frenc=[]\n",
        "for i in range(len(trans_spanish)):\n",
        "  frenc.append(model([trans_french[i]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C63M5ajP6q1Q"
      },
      "source": [
        "\n",
        "count=0\n",
        "for i in range(len(st)):\n",
        "  if (torch.argmax(st[i])==0):\n",
        "    count=count+1\n",
        "\n",
        "count_f=0\n",
        "for i in range(len(frenc)):\n",
        "  if (torch.argmax(frenc[i])==0):\n",
        "    count_f=count_f+1\n",
        "\n",
        "count_s=0\n",
        "for i in range(len(span)):\n",
        "  if (torch.argmax(span[i])==0):\n",
        "    count_s=count_s+1\n",
        "print(count_s)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRz0QD3S8tPP"
      },
      "source": [
        "count=0\n",
        "for i in range(len(tf_50_yelp)):\n",
        "  if tf_50_yelp[i]!=0:\n",
        "    count+=1;\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNfBKz9oplux"
      },
      "source": [
        "score_german=[]\n",
        "score_french=[]\n",
        "score_spanish=[]\n",
        "for i in range(len(st)):\n",
        "  if (torch.argmax(st[i])==1):\n",
        "    score_german.append(True)\n",
        "  else:\n",
        "    score_german.append(False)\n",
        "for i in range(len(frenc)):\n",
        "  if (torch.argmax(frenc[i])==1):\n",
        "    score_french.append(True)\n",
        "  else:\n",
        "    score_french.append(False)\n",
        "for i in range(len(span)):\n",
        "  if (torch.argmax(span[i])==1):\n",
        "    score_spanish.append(True)\n",
        "  else:\n",
        "    score_spanish.append(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THe block below provides how many examples are robust to NMT for a particular language (selected from score_(language)"
      ],
      "metadata": {
        "id": "pgTCh7_mlycI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW-yk1gvHL40"
      },
      "source": [
        "countt=0\n",
        "for i in range(len(score_spanish)):\n",
        "  if score_spanish[i]==True and robust_ex==0:\n",
        "    # print(org_list[i])\n",
        "    # print(i)\n",
        "    countt+=1;\n",
        "print(countt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvhgzDOxrSKW"
      },
      "source": [
        "df=pd.DataFrame({\"original\":unadv,\n",
        "                 \"adversarial\":org_list,\n",
        "                 \"back_adv_german\":trans_german,\n",
        "                 \"back_adv_french\":trans_french,\n",
        "                 \"back_adv_spanish\":trans_spanish,\n",
        "                 \"german_change\":score_german,\n",
        "                 \"french_change\":score_french,\n",
        "                 \"spanish_change\":score_spanish})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RV8Zy_68Tnq"
      },
      "source": [
        "df.to_csv('/content/drive/MyDrive/labels/file_success.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9E8dBk7Ft0Q"
      },
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/TextAttackNB/file_success.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQqs1E-3rUZm"
      },
      "source": [
        "df_vis=df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBbPvjIEM9AV"
      },
      "source": [
        "##Quantitative Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf8vrj_JM_0P"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvRtpnRINVoq"
      },
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/TextAttackNB/file_success.csv')\n",
        "\n",
        "# df_int=pd.read_csv('/content/drive/MyDrive/TextAttackNB/40cand_interpret.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NBlf0hLOXwg"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYHnKwa3Re7L"
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgVuxU06RNam"
      },
      "source": [
        "sim_metric=torch.nn.CosineSimilarity(dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY1f0I3ZRV4q"
      },
      "source": [
        "sim\n",
        "for i in range(len(df)):\n",
        "  x=df['original'][0]\n",
        "  y=df['adversarial'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndXCnKFEOULc"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlGYEr7cOVak"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S104C3zcR6pH"
      },
      "source": [
        "z=np.array(model([x]))\n",
        "a=np.array(model([y]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyMtqzXHSGHj"
      },
      "source": [
        "sent1=torch.tensor(z[0])\n",
        "sent2=torch.tensor(a[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWW44Nv4Nn02"
      },
      "source": [
        "z=sim_metric(sent1,sent2).detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NHbIOwmacjP"
      },
      "source": [
        "z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSpnN7uaSkl2"
      },
      "source": [
        "print(tf.keras.losses.cosine_similarity(\n",
        "    d,\n",
        "      f\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I__2bejGSmQG"
      },
      "source": [
        "d=model([x])\n",
        "f=model([y])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9TyJ-xeVRm6"
      },
      "source": [
        "np.sum(t)/110"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve90uGOET0UL"
      },
      "source": [
        "# USE Scores for Delete\n",
        "t=[]\n",
        "for i in range(len(df)):\n",
        "  if new_del[i]==0:\n",
        "    continue;\n",
        "  x=df['original'][i]\n",
        "  y=df['adversarial'][i]\n",
        "  sent1=torch.tensor(np.array(model([x]))[0])\n",
        "  sent2=torch.tensor(np.array(model([y]))[0])\n",
        "  sim=sim_metric(sent1,sent2)\n",
        "  t.append(sim.detach().cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZuZ0f5qapl8"
      },
      "source": [
        "int_t=[]\n",
        "for i in range(len(df)):\n",
        "  x=df_int['original'][i]\n",
        "  y=df_int['adversarial'][i]\n",
        "  sent1=torch.tensor(np.array(model([x]))[0])\n",
        "  sent2=torch.tensor(np.array(model([y]))[0])\n",
        "  sim=sim_metric(sent1,sent2)\n",
        "  int_t.append(sim.detach().cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_-6WGBbazWE"
      },
      "source": [
        "count=0;\n",
        "bf=0\n",
        "diff_list=[]\n",
        "for i in range(len(int_t)):\n",
        "  diff_list.append(int_t[i]-t[i])\n",
        "  if(diff_list[i]<=0):\n",
        "    bf+=diff_list[i]\n",
        "    count+=1;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP9aQidQa1ET"
      },
      "source": [
        "bf/count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AcesiNocT9T"
      },
      "source": [
        "import numpy as np\n",
        "def levenshtein_ratio_and_distance(s, t, ratio_calc = False):\n",
        "    \"\"\" levenshtein_ratio_and_distance:\n",
        "        Calculates levenshtein distance between two strings.\n",
        "        If ratio_calc = True, the function computes the\n",
        "        levenshtein distance ratio of similarity between two strings\n",
        "        For all i and j, distance[i,j] will contain the Levenshtein\n",
        "        distance between the first i characters of s and the\n",
        "        first j characters of t\n",
        "    \"\"\"\n",
        "    # Initialize matrix of zeros\n",
        "    rows = len(s)+1\n",
        "    cols = len(t)+1\n",
        "    distance = np.zeros((rows,cols),dtype = int)\n",
        "\n",
        "    # Populate matrix of zeros with the indeces of each character of both strings\n",
        "    for i in range(1, rows):\n",
        "        for k in range(1,cols):\n",
        "            distance[i][0] = i\n",
        "            distance[0][k] = k\n",
        "\n",
        "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    \n",
        "    for col in range(1, cols):\n",
        "        for row in range(1, rows):\n",
        "            if s[row-1] == t[col-1]:\n",
        "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
        "            else:\n",
        "                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio\n",
        "                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.\n",
        "                if ratio_calc == True:\n",
        "                    cost = 2\n",
        "                else:\n",
        "                    cost = 1\n",
        "            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions\n",
        "                                 distance[row][col-1] + 1,          # Cost of insertions\n",
        "                                 distance[row-1][col-1] + cost)     # Cost of substitutions\n",
        "    if ratio_calc == True:\n",
        "        # Computation of the Levenshtein Distance Ratio\n",
        "        Ratio = ((len(s)+len(t)) - distance[row][col]) / (len(s)+len(t))\n",
        "        return Ratio\n",
        "    else:\n",
        "        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,\n",
        "        # insertions and/or substitutions\n",
        "        # This is the minimum number of edits needed to convert string a to string b\n",
        "        return distance[row][col]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JH7AombdizT"
      },
      "source": [
        "levenshtein_ratio_and_distance(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb4-1hYjdqEY"
      },
      "source": [
        "lev_dist=[]\n",
        "for i in range(len(df)):\n",
        "  if new_del[i]==0:\n",
        "    continue;\n",
        "  x=df['original'][i]\n",
        "  y=df['adversarial'][i]\n",
        "  # d=df_int['original'][i]\n",
        "  # f=df_int['adversarial'][i]\n",
        "  lev_dist.append(levenshtein_ratio_and_distance(x,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVTIwvDmyx1H"
      },
      "source": [
        "np.sum(lev_dist)/110"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE4bU2BAoy3k"
      },
      "source": [
        "p=0\n",
        "sum=0;\n",
        "for i in range(len(lev_dist)):\n",
        "  if lev_dist[i]<0:\n",
        "    sum+=lev_dist[i]\n",
        "    p=p+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hAtUh8arYQ4"
      },
      "source": [
        "!pip install bert_score\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m35OhfDaX93W"
      },
      "source": [
        "import bert_score\n",
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZbMYgcNYL_m"
      },
      "source": [
        "from bert_score import score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gn6CLVhYN3p"
      },
      "source": [
        "P, R, F1 = score([x], [y], lang='en', verbose=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08I2JIRWYUJE"
      },
      "source": [
        "P, R, F1 = score(df['original'], df['adversarial'], lang='en', verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPmSBDV0YrDP"
      },
      "source": [
        "bert_score_del=[]\n",
        "for i in range(len(df)):\n",
        "  if new_del[i]==0:\n",
        "    continue;\n",
        "  x=df['original'][i]\n",
        "  y=df['adversarial'][i]\n",
        "  P, R, F1 = score([x], [y], lang='en', verbose=True)\n",
        "  bert_score_del.append(F1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRwjgCmY5m_x"
      },
      "source": [
        "np.sum(bert_score_del)/110"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nvlHg7c66r2"
      },
      "source": [
        "def Jaccard_Similarity(doc1, doc2): \n",
        "    \n",
        "    # List the unique words in a document\n",
        "    words_doc1 = set(doc1.lower().split()) \n",
        "    words_doc2 = set(doc2.lower().split())\n",
        "    \n",
        "    # Find the intersection of words list of doc1 & doc2\n",
        "    intersection = words_doc1.intersection(words_doc2)\n",
        "\n",
        "    # Find the union of words list of doc1 & doc2\n",
        "    union = words_doc1.union(words_doc2)\n",
        "        \n",
        "    # Calculate Jaccard similarity score \n",
        "    # using length of intersection set divided by length of union set\n",
        "    return float(len(intersection)) / len(union)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCLKPaA9678d"
      },
      "source": [
        "p=[]\n",
        "for i in range(len(df)):\n",
        "  if new_del[i]==0:\n",
        "    continue;\n",
        "  x=df['original'][i]\n",
        "  y=df['adversarial'][i]\n",
        "  p.append(Jaccard_Similarity(x,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq2EBW5N7sCq"
      },
      "source": [
        "np.sum(p)/110"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XSNYDiz7_nI"
      },
      "source": [
        "df['adversarial'][13]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg0S_Nhs8wVF"
      },
      "source": [
        "df['original'][10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r47JH2GAr57z"
      },
      "source": [
        "bert_diff_list=[]\n",
        "count_bert=0;\n",
        "c=0\n",
        "for i in range(len(bert_score_del)):\n",
        "  bert_diff_list.append(bert_score_del[i]-bert_score_int[i])\n",
        "  if bert_diff_list[i]>=0:\n",
        "    count_bert+=bert_diff_list[i]\n",
        "    c+=1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}